{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. Bayes' theorem is a fundamental theorem in probability theory that describes how to update the probability of a hypothesis, based on new evidence.\n",
        "# It relates the conditional probability of an event given the evidence to the conditional probability of the evidence given the event.\n",
        "\n",
        "# Q2. The formula for Bayes' theorem is:\n",
        "# P(A|B) = (P(B|A) * P(A)) / P(B)\n",
        "# Where:\n",
        "# P(A|B) is the posterior probability (probability of class A given the feature B),\n",
        "# P(B|A) is the likelihood (probability of observing feature B given class A),\n",
        "# P(A) is the prior probability (probability of class A),\n",
        "# P(B) is the evidence (probability of observing feature B).\n",
        "\n",
        "# Q3. Bayes' theorem is used in practice to update probabilities in light of new evidence. It is widely used in classification problems, especially for Naive Bayes classifiers, which assume that features are independent given the class label.\n",
        "\n",
        "# Q4. The relationship between Bayes' theorem and conditional probability is:\n",
        "# Bayes' theorem is used to compute the conditional probability P(A|B), which is the probability of class A given feature B.\n",
        "# Conditional probability is the probability of one event occurring given that another event has occurred, and Bayes' theorem provides a way to update this probability based on new data.\n",
        "\n",
        "# Q5. To choose which type of Naive Bayes classifier to use:\n",
        "# - Gaussian Naive Bayes is used when the features are continuous and assumed to follow a Gaussian (normal) distribution.\n",
        "# - Multinomial Naive Bayes is used for categorical features, especially for text classification where features are word counts.\n",
        "# - Bernoulli Naive Bayes is used when the features are binary.\n",
        "\n",
        "# Q6. Assignment:\n",
        "# We will use Naive Bayes classification to predict which class (A or B) the new instance with X1 = 3 and X2 = 4 belongs to.\n",
        "# The given frequency table can be used to calculate the likelihood for each class and the posterior probabilities for classes A and B.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Given frequency table\n",
        "# Frequency table for each class (A and B)\n",
        "class_A_X1 = {1: 3, 2: 3, 3: 4}\n",
        "class_A_X2 = {1: 4, 2: 3, 3: 3, 4: 3}\n",
        "\n",
        "class_B_X1 = {1: 2, 2: 2, 3: 1}\n",
        "class_B_X2 = {1: 2, 2: 2, 3: 2, 4: 3}\n",
        "\n",
        "# Assume equal prior probabilities for classes A and B (P(A) = P(B) = 0.5)\n",
        "P_A = P_B = 0.5\n",
        "\n",
        "# Calculate the likelihoods for X1=3, X2=4 for both classes\n",
        "P_X1_given_A = class_A_X1[3] / sum(class_A_X1.values())  # P(X1=3 | A)\n",
        "P_X2_given_A = class_A_X2[4] / sum(class_A_X2.values())  # P(X2=4 | A)\n",
        "\n",
        "P_X1_given_B = class_B_X1[3] / sum(class_B_X1.values())  # P(X1=3 | B)\n",
        "P_X2_given_B = class_B_X2[4] / sum(class_B_X2.values())  # P(X2=4 | B)\n",
        "\n",
        "# Apply Bayes' theorem to calculate posterior probabilities\n",
        "P_A_given_X = P_X1_given_A * P_X2_given_A * P_A\n",
        "P_B_given_X = P_X1_given_B * P_X2_given_B * P_B\n",
        "\n",
        "# Normalize to get the probabilities (total probability of evidence P(X) is the same for both classes)\n",
        "P_X = P_A_given_X + P_B_given_X\n",
        "\n",
        "P_A_given_X_normalized = P_A_given_X / P_X\n",
        "P_B_given_X_normalized = P_B_given_X / P_X\n",
        "\n",
        "# Print the predicted class based on highest posterior probability\n",
        "print(f\"Posterior probability for class A: {P_A_given_X_normalized}\")\n",
        "print(f\"Posterior probability for class B: {P_B_given_X_normalized}\")\n",
        "\n",
        "# Predict the class with the higher posterior probability\n",
        "predicted_class = 'A' if P_A_given_X_normalized > P_B_given_X_normalized else 'B'\n",
        "print(f\"The predicted class for X1=3 and X2=4 is: {predicted_class}\")\n"
      ]
    }
  ]
}